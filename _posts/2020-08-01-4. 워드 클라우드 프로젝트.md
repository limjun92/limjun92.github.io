---
title:  "4. 워드 클라우드 프로젝트"
excerpt: ""
toc: true
toc_sticky: true
categories:
  - Web
  - Crawling
tags:

last_modified_at: 2020-08-01
---

# 워드클라우드

* 데이터에서 단어 빈도를 분석하여 시각화 하는 기법
* 텍스트 데이터 필요

# 형태소 추출

* 한국어 단어에 붙는 어미와 조사를 제거하고, 단어의  
  어근만 집계되도록 하는 형태소 추출 과정이 필요하다
  
## mecab

* 한국어 자연어 처리 라이브러리

```python
from mecab import MeCab
m = MeCab()
# 객체 생성

text = "광화문역 주변에 있는 맛집을 알고 싶어요. 정보를 얻을 수 있을까요?"

print(m.morphs(text))
# 문장을 형태소로 나누어 리스트로 반환합니다
# ['광화문역', '주변', '에', '있', '는', '맛집', '을', '알', '고', '싶', '어요', '.', '정보', '를', '얻', '을', '수', '있', '을까요', '?']
print(m.nouns(text))
# 문장을 형태소로 나누고 명사만 추출한다
# ['광화문역', '주변', '맛집', '정보', '수']
print(m.pos(text))
# 문장을 형태고로 나누고 품사에 정보를 포함하여 반환한다
# [('광화문역', 'NNP'), ('주변', 'NNG'), ('에', 'JKB'), ('있', 'VV'), ('는', 'ETM'), ('맛집', 'NNG'), ('을', 'JKO'), ('알', 'VV'), ('고', 'EC'), ('싶', 'VX'), ('어요', 'EF'), ('.', 'SF'), ('정보', 'NNG'), ('를', 'JKO'), ('얻', 'VV'), ('을', 'ETM'), ('수', 'NNB'), ('있', 'VV'), ('을까요', 'EF'), ('?', 'SF')]
```

# 실습

* 단어의 빈도수를 구해서 딕셔너리 형태로

```python
# count.py
from collections import Counter
from string import punctuation
from text import data 

# 한글일 경우 아래 사용
# import mecab
# mecab = mecab.MeCab()

def count_word_freq(data) :
    _data = data.lower()
    # 소문자로 변경
    for p in punctuation :
        _data = _data.replace(p,"")
    # 특수문자 제거
    _data = _data.split()
    # 한글일 경우 아래 사용
    # _data = mecab.nouns(_data)
    
    counter = Counter(_data)
    # string 모듈의 Counter 메소드는 각 단어의 수를 세준다
    return counter
```

* from string import punctuation
      
      특수 문자가 담겨있는 함수 이다
      
---

* 워드 클라우드를 만들어 주는 함수

```python
# wc.py
from wordcloud import WordCloud
from count import count_word_freq
from text import data
from elice_utils import EliceUtils
def create_word_cloud(data) :
    counter = count_word_freq(data)
    
    cloud = WordCloud(background_color='white')
    cloud.fit_words(counter)
    
    cloud.to_file('cloud.png')
    elice_utils.send_image('cloud.png')
    
    return None
```

---

* 크롤링해서 워드클라우드 출력

```python
import requests
from bs4 import BeautifulSoup
from wc import create_word_cloud

def crawling(soup) :
    # 기사에서 내용을 추출하고 반환하세요.
    div = soup.find('div', class_="_article_body_contents")
    
    result = div.get_text().replace('\n', '').replace('// flash 오류를 우회하기 위한 함수 추가function _flash_removeCallback() {}', '').replace('\t', '')
    
    return result
    
def main() :
    
    url = "https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=001&aid=0011575988"
    req = requests.get(url)
    soup = BeautifulSoup(req.text, "html.parser")
    
    text = crawling(soup)
    create_word_cloud(text)
    
if __name__ == "__main__" :
    main()
```
