---
title:  "이진판단, 시그모이드"
excerpt: "Deep learning, 이진판단, 시그모이드"
toc: true
toc_sticky: true
categories:
  - AI_개념
tags:
  - AI
  - Deep learning
  - 이진판단
  - 시그모이드
last_modified_at: 2020-07-21
---

# 시그모이드 

임의의 실숫값 -> 함수 -> 0 ~ 1 사이의 값 출력
(범위제한 X)

어떤 확률값의 로짓(login)표현

* 로짓
** 실제 표현하려는 값을 로그값으로
** 로짓값은 상대적
** 로짓값 x를 확률값으로 변환해주는 함수
  
T1이 우승할 확률 로짓값 2  
Gen이 우승할 확률 로짓값 5  
G2가 우승할 확률 로짓값 1
  
e^5-2 = e^3 = 20.03 배  
e^2-1 = e^1 = 2.7배

* 시그모이드는 비교대상이 있어야 한다

* 내 상자의 로짓값 0.5
* 내 상자가 아닐 가능성 0

e^0.5 : e^0  
1.649 : 1

* 내 상자일 가능성
1.649/(1.649 + 1) = 0.622

* 내 상자가 아닐 가능성
1/(1.649 + 1) = 0.378

* 일반화 

1/(1 + e^-x)

* 0.51 확률로 참인 경우
정답이 거짓이 경우 파라미터 약간 수정
* 0.99 확률로 참인 경우
정답이 거짓인 경우 파라미터 대폭 수정

# Entropy

* 분자의 무질서도 or 에너지의 분산 정도
* 정보 엔트로피 -> 정보량(단위: 엔트로피) ->  불확실한 정보를 숫자로 정량화
* -> 확률의 역수에 로그를 취한 값
* 확률이 높을 수록 당연하기 때문에 정보량이 적고
* 확률이 작을 수록 정보량이 크다

## 엔트로피 기댓값
* 각 엔트로피에 '확률'을 곱해준 값
* 엔트로피를 줄여나가서 불확실성을 줄이는게 목표

## 교차 엔트로치(CCE)
엔트로피 기댓값 != 교차 엔트로피

```python 
cce_1 = -0.5*(1*np.log(0.87)+ 0.0*np.log(0.13))
print(cce_1)
```

### 시그모이드 교차 엔트로피와 편미분
1/(1 + e^-x)

x 값이 음으로 큰 값이 들어오면 계산 값 폭주






