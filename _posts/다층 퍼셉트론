비선형 활성화 함수

다층 퍼셉트론에서 필수적인 요소

활성화 함수의 도입으로 선형성의 한계를 벗어날 수 있다

sigmoid(), softmax()는 비선형 활성화 함수이다

출력계층에서는 비선형 활성화 함수를 사용하지 않고
은닉계층에서 사용한다.

은닉계층에서 선형 연산결과를 변형시켜 퍼셉트론의 출력을 만들어 낸다

복잡한 문제라고 가정 
- 퍼셉트론 수가 기하 습수적으로 늘어날 수 있다

노드수가 많은 단층 보다 노드숙 적은 다층 구조 신경망 성능이 
훨신 우수한 경우가 많다

층이 많으면 역전파를 지나치는 동안 기울기가 소멸된다
ReLu함수를 사용해서 극복가능하다

ReLu => 음수일때는 0 양수일때는 그대로 출력
np.sign()으로 ReLU의 미분값을 쉽게 구할 수 있다

