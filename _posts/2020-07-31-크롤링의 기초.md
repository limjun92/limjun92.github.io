---
title:  "크롤링의 기초"
excerpt: "requests 라이브러리, BeautifulSoup 라이브러리"
toc: true
toc_sticky: true
categories:
  - Crawling
tags:
  - requests 라이브러리
  - BeautifulSoup 라이브러리
last_modified_at: 2020-07-31
---

# 크롤링
* **웹 페이지**에서 필요한 데이터를 **추출**해내는 작업
* **크롤링**을 하는 프로그램 : 크롤러

# 크롤링을 위해 필요한 것
* 웹 페이지의 HTML을 얻기 위한 **requests**라이브러리
* HTML을 분석하기 위한 **BeautifulSoup**라이브러리

# BeautifulSoup 라이브러리
* HTML, XML, JSON 등 파일의 **구문을 분석**하는 모듈
* 웹 페이지를 표현하는 **HTML**을 분석

```python
soup = BeautifulSoup(open("index.html"), "html.parser")
```

* "html.parser"의 의미는, BeautifulSoup 객체에게  
  "html을 분석해라" 라고 알려주는 의미이다
  
## find, find_all

```python
soup.find("p")      # 처음 등장하는 태그 찾기
soup.find_all("p")  # 모든 태그 찾기

print(soup.find("p"))      #<p></p>
print(soup.find_all("p"))  #[<p></p>, <p></p>, ... , <p></p>]
```

* find, find_all 메소드를 이용하여 **HTML 태그**를 추출할 수 있다
* find는 HTML태그 **하나**를 얻는다
* find_all은 HTML태그를 여러개 담는 **리스트**를 얻는다



## class 매개변수

```python
soup.find("div")
soup.find("div", class_="test_class")
```
* **class_** 매개변수를 사용해서 특정 클래스를 가진 태그를 추출




## id 매개변수

```python
soup.find("div")
soup.find("div", id="test_class")
```

* **id** 매개변수를 사용해서 특정 id를 가진 태그를 추출

## find로 얻은 결과도 BeautifulSoup객체

```python
soup.find("div", class_="test_class").find("p")
```

* find로 얻은 결과도 **BeautifulSoup객체**이다
* find를 한 결과에 또 find를 사용할 수 있다

## get_text

```python
print(soup.find("p"))             #<p>Hello, Python!</p>
print(soup.find("p").get_test())  #Hello, Python!
```

* BeautifulSoup객체에 **get_text**메소드를 적용하면 태그가 갖고  
  있는 **텍스트**를 얻을 수 있다
  
  
# requests 라이브러리
* Python에서 HTTP 요청을 보낼 수 있는 모듈
* GET 요청 : 정보를 **조회**하기 위한 요청
* POST 요청 : 정보를 **생성, 변경**하기 위한 요청  
    
* 크롤링에서는 GET만 사용

```python
url = "https://www.google.com"
result = requests.get(url)
```

* 지정된 URL로 GET요청을 보내고 요청을 받아 처리한 후   
  result에 응답을 보낸다
  
```python
print(result.status_code)
print(result.text)
```

* 응답은 status_code로 요청의 결과를 알 수 있다
* 요청이 성공했다면 text로 해당 사이트의 HTML을 얻을 수 있다

# requests와 BeautifulSoup

```python
url = "https://www.google.com"
result = requests.get(url)
soup = BeautifulSoup(result.text, "html.parser")
```

* requests와 BeautifulSoup를 조합하여 웹 페이지의  
  HTML을 분석할 수 있다
  
# F12

* 웹에서 개발자 도구를 켤 수 있다
