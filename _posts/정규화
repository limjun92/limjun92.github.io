# 정규화

* 정규화 기법은 성능 향상을 위한 것
* 신
* 다양한 정규화

## L2
* L2손실, L1손실은 모두 절대값이 큰 파라미터에 대해 불이익을 할당
* 값의 폭주를 막고 작은 절대값의 파라미터들로 문제를 풀도록 압박
* 편향은 일반적으로 포함하지 않는다
* 주된 목적은 **과적함을 방지**시키는 것이다
---
* 가중치의 절대값이 클때 가중치의 절대값을 줄여주는 기법
* 일정 비율만큼 깍아냄
---
* λ(lambda): 정칙회율, 손실율, 정규화 항의 영향을 정하는 양의 상수

## L1

* 일률적으로 정해진 값을 덜어내는 방법
* 0혹은 0에 가까운 파라미터 값을 양산
* L2와 마찬가지로 과적합을 억제하는 효과를 가지고 있기는 하지만, 그보다 가중치 분포에 미치는 영향이 더 크다

## Drop out

* 과적합을 방지를 위해 도입되는 정규화 기법
* 계산량을 줄이기 위한 기법은 아니다
* 성능 향상을 위해 계산량을 늘려 투자하는 기법
---
* Dropout 처리 (난수 함수 처리)
* Dropout의 실직적 효과(계산량은 오히려 늘어남)
---
* 학습시점에서만 적용
* 학습 이후에는 모든 퍼셉트론들이 다시 투입
* 같은 문제를 풀더라도, 그 처리를 담당하는 퍼셉트론 그룹을 매번 다르게 배당
* 드롭아웃의 무작위 처리는 지속해서 학습과정에 교란을 가져오는데 이 덕분에 학습과정이 일종의 매너리즘에 빠지지 않고 학습
* 드롭아웃 기법은 실제 처리에 반영되는 퍼셉트론 수를 줄여버림으로서 학습을 어렵게 만드는 장애요인으로 작용하고, 학습속도를 덜어뜨리는 반작용 발생
* 과적합 현상이 줄어들고, 학습과정에서 성능과 평가 단계에서 성능의 창이가 줄어들면서 실전에서 우수한 품질을 기대할 수 있음

## Batch normalization

* 미니배치 내의 데이터들에 대해 벡터 성분별로 정규화를 수행하는 방식
* 정규화는 대상 값들에 동일한 선형 변환을 가해줌으로써 평균 0, 표준편차 1의 분포로 만들어 주는 처리  
  -> 입력 성분 간의 분포 차이로 인한 가중치 학습의 불균형을 방지하기 위해 도입
* 신경망을 깊게 쌓다보면은 역전파 처리 중 층을 거듭할 수록 파라미터 성분에 따라 기울기가 급격히 소멸 혹은 폭주하면서 학습이 제대로 이뤄지지 못하는 경우 발생  
  -> 네트워크각 층의 입력을 구성하는 성분별 분포가 심하게 달라서 발생
---
* 미니배치 데이터를 정규화 대상으로 삼는다
* 전체 데이터셋 정규화는 최초의 입력 데이터 즉, 처음에만 가능한 기법  
  - 그렇기 때문에 은닉계층에서 생산하는 은닉 벡터에 대해 적용 불가
* 미니배치 데이터 기분 평균과 표준편차를 구한 후 각각의 데이터 값에서 평균을 뺀 후 표준 편차로 나누는 정규화 과정을 거치게 되면은 성분병 불균형을 우선 해결
* 그리고 이값에다가 크기요소(가중치)라는 파라미터 값을 곱하고, 이동요소(편향)라는 파라미터 값을 거해서 성분별 불균형을 갖는 새로운 값들을 만들어 내줍니다.
* 매번 달라지는 미니배치 평균과 분산의 이용으로 일종의 잡음 주입 즉, 노이즈 효과 발생  
  신경망은 더 어렵게 학습을 진행, 미니배치 구성에 에포크마다 무작위로 바뀐다면 배치 정규화는 학습 과정을 끊임없이 교란.(드롭아웃과도 같으 효과)
  
---
합성곱 계층이나 환정연결계층 바로 다음 혹은 활성화 함수 통과 직전에 배치시켜 주는 것이 좋다
