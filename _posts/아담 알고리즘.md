# 아담 알고리즘 한줄 번역

* **Ada**ptive **m**oments

파라미터에 적용되는 실직적인 학습률을 개별 파라미터 별로 동적으로 조절해  
경사하강법의 동작을 보안하고 학습 품질을 높여주는 방법.

* 최근의 파라미터값의 변화 추세를 나타내는 정보.
* 아담 알고리즘은 모멘텀 정보와 함께 2차 모멘텀 정보까지 활용.

---

* 역전파 처리 때 해당 파라미터의 손실 기울기와 학습률을 곱한 값을  
  각 파라미터에서 빼주는 간단한 방식으로 학습을 수행
  
* 모멘텀 정보와 2차 모멘텀 정보가 전체적인 신경망 차원에서 관리되는 단일한 값이 아니다
* 이 값은 개별 파라미터 수준에서 따로 계산되고 관리,
  즉 아담 알고리즘을 이용할 결우 파라미터 하나마다 모멘텀 정보와 2차 모멘텀 정보가  
  따라붙게 되어 파라미터 관리에 필요한 메모리 소비량에 대략 3배에서 4배까지 증가
  
* 미니 배치 데이터를 처리해 학습이 이뤄질 때마다 파라미터 값들은 물론 모멘텀 정보도  
  다시 계산 즉, 계산 부담 증가
* 전체적으로 메모리 소비와 계산량이 증가하는 것은 피할 수 없다

* 파라미터별로 실질적인 학습률을 보정해 적용, 품질 저하 방지
* 파라미터를 갱신하는 마지막 순간에 학습률이 이용되게 됩니다.  
  그래서 우리가 학습률을 찾는 노력이 완전히 사라진것은 아님
