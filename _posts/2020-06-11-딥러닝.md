---
header:
  teaser: /assets/images/ai/딥러닝.PNG
title:  "머신러닝 - 딥러닝"
excerpt: "딥러닝, 역전파"
toc: true
toc_sticky: true
categories:
  - AI_개념
tags:
  - AI
  - 딥러닝
last_modified_at: 2020-06-11
---

# 1. 딥러닝(심층신경망)

* 인간의 신경망 이론
* **인공 신경망**
* 계층 구조
* 입력층과 출력층 사이에 하나이상의 은닉층을 가지고 있는 **심층 신경망** 

![딥러닝](/assets/images/ai/딥러닝.PNG)  

# 2. 인공 신경망

    인간의 뇌를 형성하는 뉴런의 집합체를 수학 모델로 표현한 것
  
* 인간의 뇌 구조 모방하여 모델링한 수학적 모델 
* 다수의 입력 => 신호를 합한하여 계산 => 출력 결정 
* 수상돌기 => 입력   세포체, 노드 => 입력합산 지점   축삭 => 출력  

![인공신경망](/assets/images/ai/인공신경망.PNG) 

## 1. 합성곱 신경망 
  * 이미지 인식(CNN기반)
## 2. 순환 신경망(Recurrent Neural Network)

    이전의 기억을 활용할 줄 알아야 한다.
    
  * 음성, text
  * LSTM(Long Short-Term Memory), GRU(Gated Recurrent Unit)로 발전
  
* * *

# 3. 딥러닝 원리
  * MCP
  
      신경세포를 이진 출력을 내는 간단한 논리 회로로 표현
      
  * perceptron
  
      입력을 받아 계산하고 출력을 반환  
    ![perceptron](/assets/images/ai/perceptron.PNG)  
  
  * 신경망은 뉴런이 여러개 모여 레이어를 구성한 후, 이 레이어가 다시 모여 구성된 형태
  * 하나의 뉴런에는 가중치와 활성화 함수가 있다
  * 입력에 가중치를 곱하고 활성화 함수를 취하면 출력값을 알수 있다
  * 학습을 통해 가중치가 변화한다, 초기에는 무작위 값을 넣어준다
  * ![활성화](/assets/images/ai/활성화.PNG)
  * error = (기대출력) - (실제출력)
  * **학습률** => 가중치 어느정도의 크기로 조정을 위한 하이퍼파라미터(적당히 작은값)
  * w(가중치) = w + X * 0.1 * error  경사하강법
  * x(입력값)이 0일 경우를 생각해서 편향(한쪽으로 치우쳐진 고정값)을 추가
  * 편향(b)
      
        입력이 0일때 아무것도 학습하지 못하는 것을 방지
  
# 4. AND OR XOR
    
    단층 퍼셉트론은 선형적이다(AND, OR을 해결할수 있지만 XOR은 해결 불가능)
    인공지능의 겨울을 가져왔었다.
      
  ## 1. 다층 퍼셉트론 => 딥러닝의 뿌리
    
      XOR 문제 해결가능
      딥러닝의 뿌리
      인공지능을 가속화 시켜준 계기
        
  ## 2. 역전파 알고리즘
  * 다층 퍼셉트론이 파라미터가 많아지면 적절한 
    가중치와 편향을 학습하기 어렵다는것을 해결
  
# 5. 신경망의 목적
  * 손실함수가 최솟값일때의 파라미터를 찾아 올바른 학습 결과를 내는 것
  * 회귀분석, 로지스틱회귀와 기본개념이 같다
  * 회귀분석보다 파라미터수가 더 많은 편

# 6. 역전파

  * **뉴런의 가중치를 효율적을 조정**하기 위하여 **거꾸로 무엇인가를 전파하는 방식**
  * 순전파 & 역전파  
  * 출력값과 지도 데이터 사이의 오차를 이용해 **출력층에서 입력층쪽으로 가중치**를 조정한다  
  ![역전파1](/assets/images/ai/역전파1.PNG)  
  ![역전파2](/assets/images/ai/역전파2.PNG)  
  
  * 역전파는 경사하강법을 사용하는 것이기도 한다
  * 특정 입력값에서 손실함수 최솟값은 크게 의미가 없다
  * 모든 입력값을 대상으로 손실함수가 최솟값일 때의 파라미터를 찾는 것이 목표
  * E를 가중치 W에 관하여 편미분한다
  * 기울기가 0이라고해도 그 값이 최소값이라고 할 수 없다
  * 입력값 각각을 편미분한후 합이 0에 가까운지 확인  
  ![역전파3](/assets/images/ai/역전파3.PNG)    
  * 수열의 점화식처럼 오차를 전파하는 방법이기도 하다
  * 역방향 미분이라고도 한다  
  * 시그모이드, 소프트맥스 =>  기울기 소멸(신경망이 깊어질수록 학습이 잘 되지 않는 문제가 발생)
  * (ReLU 입력이 음수-> 0), (입력이 양수 -> 값 그대로) => 기울기 소명에 어느정도 면역
